{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thrown-funeral",
   "metadata": {},
   "source": [
    "# 1) Sine function generation\n",
    "In this first draft, we will try to generate sine values from our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-alarm",
   "metadata": {},
   "source": [
    "## 1) Preparing the data\n",
    "In this first part, we'll generate a dataset we can use to train our networks. It will consist in 1024 tuples, representing a sine function. These data are easy to create as we know a formula to generate an infinite amount of them.\n",
    "\n",
    "Firstly, we'll set a hardcoded seed to be able to reproduce our experience easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fixed seed to run similar experiments throughout the tests\n",
    "torch.manual_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet of size 1024\n",
    "train_data_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.zeros((train_data_length, 2))  # Create a 1024x2 matrix\n",
    "train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)  # Constraint the first values between 0 and 2*PI\n",
    "train_data[:, 1] = torch.sin(train_data[:, 0])  # Set the second value to the sine of the first value\n",
    "train_labels = torch.zeros(train_data_length)\n",
    "\n",
    "train_set = [\n",
    "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-terrace",
   "metadata": {},
   "source": [
    "Here we can plot the data we have using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_data[:, 0], train_data[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-gibson",
   "metadata": {},
   "source": [
    "The following bit creates random sample of 32 elements, used to train the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-memphis",
   "metadata": {},
   "source": [
    "## The discriminator\n",
    "The follwing class implements the discriminator part of our project. It consists in a sequential neural network.\n",
    "![DiscriminatorDiagram](../Diagrams/SineTest/SineDiscriminator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-presentation",
   "metadata": {},
   "source": [
    "## 3) The Generator\n",
    "Now we will implement the central piece of our GAN, the Generator. It is also a sequential Network, but with diffrences in its architecture\n",
    "![DiscriminatorDiagram](../Diagrams/SineTest/SineGenerator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-tolerance",
   "metadata": {},
   "source": [
    "## 4) Training\n",
    "\n",
    "#### Training Parameters\n",
    "To train our networks, we'll need to set some values.\n",
    "\n",
    "* lr (Learning Rate):\n",
    "\n",
    "This value represents the weight of the modification to apply during the backpropagation. The higher this rate is, the more the weights of the neurons will be changed. \n",
    "\n",
    "* epochs:\n",
    "\n",
    "The number of training iterations\n",
    "\n",
    "* Loss Function:\n",
    "\n",
    "The function used to compute the error rate of the network in order to update the weights of the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 500\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-prototype",
   "metadata": {},
   "source": [
    "#### Pytorch Optimizator\n",
    "The next lines set an optimizer to be used during gradient calculations. We'll use the Adam optimizer, derivated from the \"Adaptive Moment Optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-christian",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        # Data for training the discriminator\n",
    "        real_samples_labels = torch.ones((batch_size, 1))\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        # Training the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels)\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # Data for training the generator\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "\n",
    "        # Training the generator\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Show loss\n",
    "        if epoch % 100 == 0 and n == batch_size - 1:\n",
    "            latent_space_samples = torch.randn(100, 2)\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            generated_samples = generated_samples.detach()\n",
    "            plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n",
    "    plt.legend([f'Iteration {x}' for x in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-benchmark",
   "metadata": {},
   "source": [
    "## 5) Results\n",
    "Now we will try to see if the generator has trained correctly by making him generate data, and showing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_samples = torch.randn(300, 2)  # Random noise vectors\n",
    "generated_samples = generator(latent_space_samples)  # Generated sine vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-insertion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
